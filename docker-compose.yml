# =============================================================================
# Docker Compose - Multi-Container Orchestration
# =============================================================================
# Services:
#   - mlflow: Experiment tracking server
#   - training: GPU/CPU training container
#   - gradio: Web interface
#   - minio: S3-compatible storage for artifacts (optional)
# =============================================================================
# Usage:
#   docker-compose up -d                    # Start all services
#   docker-compose up mlflow minio          # Start MLFlow + storage only
#   docker-compose --profile gpu up         # Start with GPU support
#   docker-compose logs -f training         # View training logs
# =============================================================================

version: "3.9"

services:
  # ---------------------------------------------------------------------------
  # MLFlow Tracking Server
  # ---------------------------------------------------------------------------
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.0
    container_name: eo-mlflow
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
      - MLFLOW_HOST=0.0.0.0
    volumes:
      - mlflow-data:/mlflow
      - ./models:/mlflow/artifacts/models
    command: >
      mlflow server 
        --backend-store-uri sqlite:///mlflow/mlflow.db 
        --default-artifact-root /mlflow/artifacts 
        --host 0.0.0.0 
        --port 5000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------------------------------------------------------------------------
  # MinIO - S3-Compatible Object Storage (for DVC & MLFlow artifacts)
  # ---------------------------------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: eo-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------------------------------------------------------------------------
  # Training Container (CPU)
  # ---------------------------------------------------------------------------
  training:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BASE_IMAGE=python:3.12-slim
    container_name: eo-training
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - DVC_REMOTE=minio
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY:-minioadmin}
      - AWS_ENDPOINT_URL=http://minio:9000
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./configs:/app/configs
      - ./outputs:/app/outputs
    depends_on:
      - mlflow
      - minio
    profiles:
      - training

  # ---------------------------------------------------------------------------
  # Training Container (GPU) - Use with --profile gpu
  # ---------------------------------------------------------------------------
  training-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BASE_IMAGE=nvidia/cuda:12.1-cudnn8-runtime-ubuntu22.04
    container_name: eo-training-gpu
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - DVC_REMOTE=minio
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY:-minioadmin}
      - AWS_ENDPOINT_URL=http://minio:9000
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./configs:/app/configs
      - ./outputs:/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - mlflow
      - minio
    profiles:
      - gpu

  # ---------------------------------------------------------------------------
  # Gradio Web Interface
  # ---------------------------------------------------------------------------
  gradio:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BASE_IMAGE=python:3.12-slim
    container_name: eo-gradio
    ports:
      - "7860:7860"
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./models/production:/app/models/production:ro
    command: python app/gradio_app.py
    depends_on:
      - mlflow
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Jupyter Lab (Development)
  # ---------------------------------------------------------------------------
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: eo-jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-eo_pipeline}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - .:/app
    command: >
      jupyter lab 
        --ip=0.0.0.0 
        --port=8888 
        --no-browser 
        --allow-root
        --NotebookApp.token=${JUPYTER_TOKEN:-eo_pipeline}
    profiles:
      - dev

# =============================================================================
# Volumes
# =============================================================================
volumes:
  mlflow-data:
    name: eo-mlflow-data
  minio-data:
    name: eo-minio-data

# =============================================================================
# Networks (optional - uncomment for isolated networking)
# =============================================================================
# networks:
#   eo-network:
#     driver: bridge
